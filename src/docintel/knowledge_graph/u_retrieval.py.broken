"""
U-Retrieval System for Clinical Knowledge Graphs

Implements hierarchical retrieval system that leverages community structure 
for more precise and context-aware query results per Medical-Graph-RAG patterns.

Features:
- Global-to-local search strategy
- Community-aware ranking
- Multi-level context aggregation
- Clinical vocabulary integration
- Semantic similarity scoring
"""

import logging
import asyncio
import json
from typing import List, Dict, Any, Optional, Tuple, Set
from dataclasses import dataclass, asdict
from enum import Enum
import numpy as np
from collections import defaultdict, Counter

import psycopg
from ..config import get_config

logger = logging.getLogger(__name__)


class QueryType(Enum):
    """Types of clinical queries supported"""
    ENTITY_SEARCH = "entity_search"
    RELATION_SEARCH = "relation_search"
    COMMUNITY_SEARCH = "community_search"
    SEMANTIC_SEARCH = "semantic_search"
    HYBRID_SEARCH = "hybrid_search"


class SearchScope(Enum):
    """Search scope levels"""
    GLOBAL = "global"           # Search across all communities
    COMMUNITY = "community"     # Search within specific communities
    LOCAL = "local"            # Search within specific documents/chunks


@dataclass
class QueryContext:
    """Context information for queries"""
    clinical_area: Optional[str] = None      # e.g., "oncology", "cardiology"
    study_phase: Optional[str] = None        # e.g., "phase_1", "phase_2"
    entity_types: List[str] = None          # Filter by entity types
    vocabularies: List[str] = None          # Filter by vocabularies (UMLS, SNOMED, etc.)
    confidence_threshold: float = 0.0       # Minimum confidence threshold
    
    def __post_init__(self):
        if self.entity_types is None:
            self.entity_types = []
        if self.vocabularies is None:
            self.vocabularies = []


@dataclass
class SearchResult:
    """Individual search result with context"""
    entity_id: str
    entity_text: str
    entity_type: str
    normalized_concept_id: Optional[str]
    normalized_vocabulary: Optional[str]
    confidence: float
    community_id: Optional[str]
    community_title: Optional[str]
    chunk_id: str
    document_context: str
    relevance_score: float
    explanation: str
    metadata: Dict[str, Any]


@dataclass
class URetrievalResult:
    """Complete U-Retrieval result with hierarchical context"""
    query: str
    query_type: QueryType
    search_scope: SearchScope
    results: List[SearchResult]
    community_aggregation: Dict[str, Any]
    global_context: Dict[str, Any]
    processing_stats: Dict[str, Any]
    total_results: int
    processing_time_ms: float


class ClinicalURetrieval:
    """
    U-Retrieval system for clinical knowledge graphs.
    
    Implements hierarchical retrieval leveraging:
    - Community structure for context-aware search
    - Normalized entities for vocabulary-aware matching
    - Multi-level aggregation for comprehensive results
    - Clinical domain expertise for relevance scoring
    """
    
    def __init__(self, connection_string: str):
        self.connection_string = connection_string
        self.conn = None
        
        # Clinical relevance weights for different entity types
        self.entity_type_weights = {
            'drug': 1.0,
            'medication': 1.0,
            'disease': 0.9,
            'condition': 0.9,
            'symptom': 0.8,
            'adverse_event': 0.8,
            'procedure': 0.7,
            'measurement': 0.6,
            'population': 0.5,
            'temporal': 0.4
        }
        
        # Vocabulary authority weights
        self.vocabulary_weights = {
            'rxnorm': 1.0,      # Authoritative for medications
            'snomed': 0.9,      # Comprehensive clinical terminology
            'umls': 0.8,        # Broad medical coverage
            'icd10': 0.7,       # Diagnostic codes
            'loinc': 0.6        # Laboratory terms
        }
    
    async def connect(self):
        """Establish database connection"""
        self.conn = await psycopg.AsyncConnection.connect(self.connection_string)
        await self.conn.execute("LOAD 'age';")
        await self.conn.execute('SET search_path = ag_catalog, public;')
        logger.info("Connected to database and loaded AGE extension")
    
    async def close(self):
        """Close database connection"""
        if self.conn:
            await self.conn.close()
    
    async def u_retrieval_search(
        self,
        query: str,
        query_type: QueryType = QueryType.HYBRID_SEARCH,
        search_scope: SearchScope = SearchScope.GLOBAL,
        context: Optional[QueryContext] = None,
        max_results: int = 50
    ) -> URetrievalResult:
        """
        Perform U-Retrieval search with hierarchical community-aware ranking.
        
        Args:
            query: Search query text
            query_type: Type of search to perform
            search_scope: Scope of search (global, community, local)
            context: Additional query context
            max_results: Maximum number of results to return
            
        Returns:
            URetrievalResult with hierarchical search results
        """
        start_time = asyncio.get_event_loop().time()
        
        if context is None:
            context = QueryContext()
        
        logger.info(f"Starting U-Retrieval search: '{query}' (type: {query_type.value}, scope: {search_scope.value})")
        
        try:
            await self.connect()
            
            # Step 1: Global search - find relevant communities
            relevant_communities = await self._find_relevant_communities(query, context)
            logger.info(f"Found {len(relevant_communities)} relevant communities")
            
            # Step 2: Community-aware entity search
            entity_results = await self._community_aware_entity_search(
                query, relevant_communities, context, max_results
            )
            logger.info(f"Found {len(entity_results)} entity matches")
            
            # Step 3: Relation-aware expansion (if hybrid search)
            if query_type in [QueryType.RELATION_SEARCH, QueryType.HYBRID_SEARCH]:
                relation_results = await self._relation_aware_expansion(
                    query, entity_results, context
                )
                entity_results.extend(relation_results)
                logger.info(f"Expanded to {len(entity_results)} results with relations")
            
            # Step 4: Community aggregation and ranking
            ranked_results = await self._community_aware_ranking(
                entity_results, relevant_communities, query
            )
            
            # Step 5: Global context aggregation
            global_context = await self._aggregate_global_context(
                ranked_results, relevant_communities
            )
            
            # Step 6: Community-level aggregation
            community_aggregation = await self._aggregate_community_context(
                ranked_results, relevant_communities
            )
            
            end_time = asyncio.get_event_loop().time()
            processing_time_ms = (end_time - start_time) * 1000
            
            # Create final result
            result = URetrievalResult(
                query=query,
                query_type=query_type,
                search_scope=search_scope,
                results=ranked_results[:max_results],
                community_aggregation=community_aggregation,
                global_context=global_context,
                processing_stats={
                    "communities_searched": len(relevant_communities),
                    "raw_results": len(entity_results),
                    "final_results": min(len(ranked_results), max_results),
                    "processing_steps": ["community_discovery", "entity_search", "relation_expansion", "ranking", "aggregation"]
                },
                total_results=len(ranked_results),
                processing_time_ms=processing_time_ms
            )
            
            logger.info(f"U-Retrieval search completed in {processing_time_ms:.1f}ms")
            return result
            
        finally:
            await self.close()
    
    async def _find_relevant_communities(
        self, 
        query: str, 
        context: QueryContext
    ) -> List[Dict[str, Any]]:
        """Find communities relevant to the query"""
        
        # Get all communities with their metadata (not just top by occurrence)
        community_query = """
            SELECT 
                cluster_key,
                level,
                title,
                nodes,
                edges,
                chunk_ids,
                occurrence,
                created_at
            FROM ag_catalog.communities
            ORDER BY cluster_key
        """
        
        result = await self.conn.execute(community_query)
        communities = await result.fetchall()
        
        if not communities:
            logger.warning("No communities found in database")
            return []
        
        relevant_communities = []
        query_lower = query.lower()
        
        # Filter out common words that don't add meaning
        stop_words = {'and', 'or', 'the', 'a', 'an', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
        meaningful_terms = [term for term in query_lower.split() if term not in stop_words and len(term) > 2]
        
        logger.info(f"Searching communities for meaningful terms: {meaningful_terms}")
        
        for community in communities:
            cluster_key, level, title, nodes, edges, chunk_ids, occurrence, created_at = community
            
            # Parse JSON fields
            nodes_list = json.loads(nodes) if isinstance(nodes, str) else nodes
            edges_list = json.loads(edges) if isinstance(edges, str) else edges
            chunk_ids_list = json.loads(chunk_ids) if isinstance(chunk_ids, str) else chunk_ids
            
            # Calculate relevance score for this community
            relevance_score = 0.0
            
            # Check title relevance with meaningful terms
            if title and any(term in title.lower() for term in meaningful_terms):
                relevance_score += 0.3
            
            # Check if community contains entities matching meaningful query terms
            entity_match_score = await self._calculate_community_entity_relevance(
                chunk_ids_list, query_lower, meaningful_terms
            )
            relevance_score += entity_match_score * 0.7
            
            # Boost relevance for safety/clinical terms
            clinical_terms = {'adverse', 'safety', 'monitor', 'effect', 'event', 'risk'}
            if any(term in meaningful_terms for term in clinical_terms):
                relevance_score *= 1.5
            
            # Don't weight by occurrence to avoid missing relevant but smaller communities
            
            if relevance_score > 0.05:  # Lower threshold for relevance
                relevant_communities.append({
                    'cluster_key': cluster_key,
                    'level': level,
                    'title': title,
                    'nodes': nodes_list,
                    'edges': edges_list,
                    'chunk_ids': chunk_ids_list,
                    'occurrence': occurrence,
                    'relevance_score': relevance_score
                })
        
        # Sort by relevance score
        relevant_communities.sort(key=lambda x: x['relevance_score'], reverse=True)
        
        # Return top communities (limit to avoid too many)
        return relevant_communities[:10]
    
    async def _calculate_community_entity_relevance(
        self, 
        chunk_ids: List[str], 
        query_lower: str,
        meaningful_terms: List[str] = None
    ) -> float:
        """Calculate how well community entities match the query"""
        if not chunk_ids:
            return 0.0
        
        # Get entity texts for entities in these chunks
        chunk_placeholders = ','.join(['%s'] * len(chunk_ids))
        entity_query = f"""
            SELECT entity_text, entity_type, normalized_id
            FROM docintel.entities 
            WHERE chunk_id::text IN ({chunk_placeholders})
        """
        
        result = await self.conn.execute(entity_query, chunk_ids)
        entities = await result.fetchall()
        
        if not entities:
            return 0.0
        
        relevance_score = 0.0
        search_terms = meaningful_terms if meaningful_terms else query_lower.split()
        
        for entity_text, entity_type, normalized_id in entities:
            entity_lower = entity_text.lower()
            
            # Exact phrase match bonus
            if any(term in entity_lower for term in search_terms if len(term) > 3):
                relevance_score += 0.5
            
            # Partial match scoring for meaningful terms
            for term in search_terms:
                if len(term) > 2 and term in entity_lower:
                    relevance_score += 0.3
                    
                    # Extra weight for clinical terms
                    if term in {'adverse', 'safety', 'monitor', 'effect', 'event', 'risk'}:
                        relevance_score += 0.4
            
            # Entity type weighting
            type_weight = self.entity_type_weights.get(entity_type, 0.3)
            relevance_score *= type_weight
        
        # Normalize by number of entities
        return relevance_score / len(entities)
    
    async def _community_aware_entity_search(
        self,
        query: str,
        relevant_communities: List[Dict[str, Any]],
        context: QueryContext,
        max_results: int
    ) -> List[SearchResult]:
        """Search for entities with community awareness"""
        
        search_results = []
        query_lower = query.lower()
        query_terms = query_lower.split()
        
        # Get all chunk IDs from relevant communities (use chunk_ids instead of nodes)
        all_chunk_ids = set()
        chunk_to_community = {}  # chunk_id -> community info
        
        for community in relevant_communities:
            for chunk_id in community['chunk_ids']:
                all_chunk_ids.add(str(chunk_id))
                chunk_to_community[str(chunk_id)] = community
        
        if not all_chunk_ids:
            logger.warning("No chunk IDs found in relevant communities")
            return []
        
        # Search entities by chunk_id with normalization data
        # Search entities from these meta_graphs
        meta_graph_list = list(meta_graph_ids)
        placeholders = ','.join(['%s'] * len(meta_graph_list))
        
        search_query = f"""
            SELECT 
                e.entity_id,
                e.entity_text,
                e.entity_type,
                e.confidence,
                e.normalized_id,
                e.normalized_source,
                e.context_flags,
                e.chunk_id,
                e.source_chunk_id,
                e.meta_graph_id
            FROM docintel.entities e
            WHERE e.meta_graph_id::text IN ({placeholders})
            AND (
                LOWER(e.entity_text) LIKE %s
                OR LOWER(e.entity_type) LIKE %s
                OR e.normalized_id IS NOT NULL
            )
            ORDER BY e.confidence DESC
            LIMIT 500
        """
        
        # Create search patterns
        search_pattern = f"%{query_lower}%"
        params = meta_graph_list + [search_pattern, search_pattern]
        
        result = await self.conn.execute(search_query, params)
        entities = await result.fetchall()
        
        # Create search patterns
        search_pattern = f"%{query_lower}%"
        params = chunk_ids_list + [search_pattern, search_pattern]
        
        result = await self.conn.execute(search_query, params)
        entities = await result.fetchall()
        
        for entity in entities:
            (entity_id, entity_text, entity_type, confidence, normalized_id, 
             normalized_source, context_flags, chunk_id) = entity
            
            # Calculate relevance score
            relevance_score = self._calculate_entity_relevance_score(
                entity_text, entity_type, query_terms, confidence, normalized_source
            )
            
            # Get community context from chunk mapping
            community_info = chunk_to_community.get(str(chunk_id), {})
            
            # Create search result
            search_result = SearchResult(
                entity_id=str(entity_id),
                entity_text=entity_text,
                entity_type=entity_type,
                normalized_concept_id=normalized_id,
                normalized_vocabulary=normalized_source,
                confidence=confidence or 0.0,
                community_id=community_info.get('cluster_key'),
                community_title=community_info.get('title'),
                chunk_id=str(chunk_id) if chunk_id else "",
                document_context="",  # Will be filled later if needed
                relevance_score=relevance_score,
                explanation=f"Found in community '{community_info.get('title', 'Unknown')}' with relevance {relevance_score:.3f}",
                metadata={
                    'community_occurrence': community_info.get('occurrence', 0.0),
                    'community_level': community_info.get('level', 0),
                    'context_flags': json.loads(context_flags) if context_flags else {},
                    'normalized_id': normalized_id,
                    'normalized_source': normalized_source
                }
            )
            
            search_results.append(search_result)
        
        logger.info(f"Found {len(search_results)} entities matching query")
        return search_results
    
    def _calculate_entity_relevance_score(
        self,
        entity_text: str,
        entity_type: str,
        query_terms: List[str],
        confidence: float,
        normalized_source: Optional[str]
    ) -> float:
        """Calculate relevance score for an entity"""
        
        score = 0.0
        entity_lower = entity_text.lower()
        
        # Text matching score
        for term in query_terms:
            if term == entity_lower:
                score += 1.0  # Exact match
            elif term in entity_lower:
                score += 0.5  # Partial match
        
        # Entity type weighting
        type_weight = self.entity_type_weights.get(entity_type, 0.3)
        score *= type_weight
        
        # Confidence weighting
        conf_weight = confidence if confidence else 0.5
        score *= conf_weight
        
        # Vocabulary authority weighting
        if normalized_source:
            vocab_weight = self.vocabulary_weights.get(normalized_source.lower(), 0.5)
            score *= vocab_weight
        
        return score
    
    async def _relation_aware_expansion(
        self,
        query: str,
        entity_results: List[SearchResult],
        context: QueryContext
    ) -> List[SearchResult]:
        """Expand results using relation information"""
        
        if not entity_results:
            return []
        
        expanded_results = []
        entity_ids = [result.entity_id for result in entity_results[:20]]  # Limit for performance
        
        if not entity_ids:
            return []
        
        # Find relations involving these entities
        placeholders = ','.join(['%s'] * len(entity_ids))
        relation_query = f"""
            SELECT DISTINCT
                r.subject_entity_id,
                r.object_entity_id,
                r.predicate,
                r.confidence,
                r.evidence_span,
                se.entity_text as subject_text,
                se.entity_type as subject_type,
                oe.entity_text as object_text,
                oe.entity_type as object_type
            FROM docintel.relations r
            JOIN docintel.entities se ON r.subject_entity_id = se.entity_id
            JOIN docintel.entities oe ON r.object_entity_id = oe.entity_id
            WHERE r.subject_entity_id::text IN ({placeholders})
               OR r.object_entity_id::text IN ({placeholders})
            ORDER BY r.confidence DESC NULLS LAST
            LIMIT 100
        """
        
        result = await self.conn.execute(relation_query, entity_ids + entity_ids)
        relations = await result.fetchall()
        
        query_lower = query.lower()
        
        for relation in relations:
            (subject_id, object_id, predicate, confidence, evidence_span,
             subject_text, subject_type, object_text, object_type) = relation
            
            # Check if relation is relevant to query
            relation_text = f"{subject_text} {predicate} {object_text}".lower()
            if any(term in relation_text for term in query_lower.split()):
                
                # Create expanded result for the related entity not in original results
                related_entity_id = object_id if subject_id in entity_ids else subject_id
                related_text = object_text if subject_id in entity_ids else subject_text
                related_type = object_type if subject_id in entity_ids else subject_type
                
                expanded_result = SearchResult(
                    entity_id=str(related_entity_id),
                    entity_text=related_text,
                    entity_type=related_type,
                    normalized_concept_id=None,
                    normalized_vocabulary=None,
                    confidence=confidence or 0.0,
                    community_id=None,
                    community_title=None,
                    chunk_id="",
                    document_context=evidence_span or "",
                    relevance_score=0.3 * (confidence or 0.5),  # Lower score for expanded results
                    explanation=f"Related via '{predicate}' to primary search results",
                    metadata={
                        'relation_type': 'expanded',
                        'predicate': predicate,
                        'evidence_span': evidence_span
                    }
                )
                
                expanded_results.append(expanded_result)
        
        logger.info(f"Expanded {len(entity_results)} results to {len(entity_results) + len(expanded_results)} with relations")
        return expanded_results
    
    async def _community_aware_ranking(
        self,
        results: List[SearchResult],
        relevant_communities: List[Dict[str, Any]],
        query: str
    ) -> List[SearchResult]:
        """Rank results using community context"""
        
        # Create community relevance lookup
        community_relevance = {
            comm['cluster_key']: comm['relevance_score'] 
            for comm in relevant_communities
        }
        
        # Enhance ranking with community context
        for result in results:
            # Base relevance score
            final_score = result.relevance_score
            
            # Community boost
            if result.community_id and result.community_id in community_relevance:
                community_boost = community_relevance[result.community_id] * 0.3
                final_score += community_boost
                
                # Update explanation
                result.explanation += f" (community boost: +{community_boost:.3f})"
            
            # Update final relevance score
            result.relevance_score = final_score
        
        # Sort by enhanced relevance score
        results.sort(key=lambda x: x.relevance_score, reverse=True)
        
        return results
    
    async def _aggregate_global_context(
        self,
        results: List[SearchResult],
        relevant_communities: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Aggregate global context from search results"""
        
        # Entity type distribution
        entity_types = Counter(result.entity_type for result in results)
        
        # Vocabulary distribution
        vocabularies = Counter(
            result.normalized_vocabulary for result in results 
            if result.normalized_vocabulary
        )
        
        # Community distribution
        communities = Counter(
            result.community_title for result in results 
            if result.community_title
        )
        
        # Confidence statistics
        confidences = [result.confidence for result in results if result.confidence > 0]
        avg_confidence = np.mean(confidences) if confidences else 0.0
        
        return {
            'total_entities': len(results),
            'unique_entity_types': len(entity_types),
            'entity_type_distribution': dict(entity_types.most_common(10)),
            'vocabulary_distribution': dict(vocabularies.most_common()),
            'community_distribution': dict(communities.most_common()),
            'average_confidence': float(avg_confidence),
            'communities_involved': len(relevant_communities),
            'search_coverage': {
                'entities_with_normalization': sum(1 for r in results if r.normalized_concept_id),
                'entities_with_communities': sum(1 for r in results if r.community_id),
                'entities_with_relations': sum(1 for r in results if r.metadata.get('relation_type'))
            }
        }
    
    async def _aggregate_community_context(
        self,
        results: List[SearchResult],
        relevant_communities: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Aggregate community-level context"""
        
        community_results = defaultdict(list)
        
        # Group results by community
        for result in results:
            if result.community_id:
                community_results[result.community_id].append(result)
        
        community_summaries = {}
        
        for community_id, community_results_list in community_results.items():
            # Find community info
            community_info = next(
                (c for c in relevant_communities if c['cluster_key'] == community_id), 
                {}
            )
            
            # Calculate community-specific statistics
            entity_types = Counter(r.entity_type for r in community_results_list)
            avg_relevance = np.mean([r.relevance_score for r in community_results_list])
            
            community_summaries[community_id] = {
                'title': community_info.get('title', f'Community {community_id}'),
                'total_results': len(community_results_list),
                'average_relevance': float(avg_relevance),
                'entity_types': dict(entity_types),
                'occurrence': community_info.get('occurrence', 0.0),
                'level': community_info.get('level', 0)
            }
       